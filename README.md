# LLM Evaluation Framework with MLflow

A comprehensive framework for evaluating Large Language Model (LLM) responses using MLflow tracking and answer similarity metrics. This system compares responses from local models (like Ollama) against reference responses generated by a judge model (GPT-4).

## ğŸ—ï¸ Architecture Overview

This framework consists of two main components that work together to provide automated LLM evaluation:

```mermaid
graph TB
    subgraph "Main Application (main.py)"
        A[Environment Setup] --> B[Ollama Client]
        B --> C[Response Generation]
        C --> D[Evaluation Orchestration]
    end
    
    subgraph "Evaluation Engine (inference_tracking.py)"
        E[LLMEvaluator Class] --> F[MLflow Setup]
        F --> G[Judge Model Client]
        G --> H[Expected Response Generation]
        H --> I[Answer Similarity Evaluation]
    end
    
    subgraph "External Services"
        J[Ollama Server]
        K[OpenAI API]
        L[MLflow Tracking Server]
    end
    
    D --> E
    B --> J
    G --> K
    I --> L
    
    style A fill:#e1f5fe
    style E fill:#f3e5f5
    style J fill:#fff3e0
    style K fill:#fff3e0
    style L fill:#fff3e0
```

## ğŸ“‹ Components Documentation

### 1. Main Application (`main.py`)

The main application handles the orchestration of the evaluation process and manages the interaction with the Ollama model.

#### Key Functions:

- **`check_env_vars()`**: Validates all required environment variables
- **`chat_with_ollama(messages, client)`**: Handles individual chat requests to Ollama
- **`generate_responses_with_ollama(questions)`**: Generates responses for multiple questions
- **`main()`**: Main execution function that orchestrates the entire evaluation process

#### Responsibilities:
- Environment validation and setup
- Ollama client configuration and management
- Question batch processing
- Response generation using local LLM
- Evaluation result presentation

### 2. Evaluation Engine (`inference_tracking.py`)

The evaluation engine contains the core evaluation logic and manages the judge model for generating reference responses.

#### LLMEvaluator Class:

```python
class LLMEvaluator:
    def __init__(openai_api_key, openai_model="gpt-4", ...)
    def setup_mlflow(tracking_uri, experiment_name, ...)
    def generate_expected_response_with_judge_model(question)
    def generate_expected_responses(questions)
    def evaluate_responses(questions, generated_responses, ...)
```

#### Key Features:
- **Judge Model Integration**: Uses GPT-4 as a reference model to generate expected responses
- **MLflow Integration**: Comprehensive experiment tracking and metrics logging
- **Answer Similarity Evaluation**: Uses MLflow's answer similarity metric for evaluation
- **Flexible Configuration**: Customizable temperature, token limits, and model selection

## ğŸ”„ Sequence Diagram

The following diagram shows the complete evaluation flow:

```mermaid
sequenceDiagram
    participant User
    participant Main as main.py
    participant Evaluator as LLMEvaluator
    participant Ollama as Ollama Server
    participant Judge as GPT-4 (Judge)
    participant MLflow as MLflow Server

    User->>Main: Run evaluation
    Main->>Main: check_env_vars()
    Main->>Evaluator: Initialize LLMEvaluator
    Main->>Evaluator: setup_mlflow()
    Evaluator->>MLflow: Configure experiment & tracking
    
    loop For each question
        Main->>Ollama: generate_responses_with_ollama()
        Ollama-->>Main: Generated response
    end
    
    Main->>Evaluator: evaluate_responses()
    
    loop For each question (if use_judge_model=True)
        Evaluator->>Judge: generate_expected_response()
        Judge-->>Evaluator: Expected response
    end
    
    Evaluator->>Evaluator: Create evaluation DataFrame
    Evaluator->>MLflow: Start MLflow run
    Evaluator->>MLflow: mlflow.evaluate() with answer_similarity
    MLflow-->>Evaluator: Evaluation results & metrics
    Evaluator->>MLflow: Log parameters & metrics
    Evaluator->>MLflow: End run
    
    Evaluator-->>Main: Return evaluation results
    Main-->>User: Display results & metrics
```

## ğŸš€ Installation & Setup

### Prerequisites

```bash
pip install mlflow openai pandas httpx python-dotenv
```

### Environment Variables

Create a `.env` file with the following variables:

```env
# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4

# Ollama Configuration  
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_MODEL=llama3.1:8b
OPENAPI_KEY=ollama  # Can be any value for Ollama

# MLflow Configuration
MLFLOW_TRACKING_URI=http://localhost:5000
MLFLOW_EXPERIMENT_NAME=llm_evaluation
MLFLOW_ACTIVE_MODEL_NAME=llama318b_model
```

## ğŸ’» Usage

### Basic Usage

```python
python main.py
```

### Custom Usage

```python
from inference_tracking import LLMEvaluator

# Initialize evaluator
evaluator = LLMEvaluator(
    openai_api_key="your_key",
    openai_model="gpt-4",
    teacher_model_temperature=0.7,
    teacher_model_max_tokens=1500
)

# Setup MLflow
evaluator.setup_mlflow(
    tracking_uri="http://localhost:5000",
    experiment_name="my_experiment"
)

# Evaluate responses
results = evaluator.evaluate_responses(
    questions=["Your questions here"],
    generated_responses=["Model responses here"],
    use_judge_model=True
)
```

## ğŸ“Š Evaluation Metrics

The framework uses MLflow's answer similarity metric, which:

- **Measures semantic similarity** between generated and expected responses
- **Uses GPT-4 as judge** for nuanced evaluation beyond simple text matching
- **Returns scores** typically between 0-1 or 1-5 depending on configuration
- **Provides detailed evaluation tables** with per-question breakdowns

### Sample Output
EVALUATION RESULTS
==================================================
Run ID: a1b2c3d4e5f6...
Metrics: {
'answer_similarity/v1/score': 0.85,
'answer_similarity/v1/mean': 0.85,
'answer_similarity/v1/variance': 0.02
}
Answer Similarity Score: 0.85


## ğŸ¯ Key Features

- **ğŸ”„ Automated Evaluation**: End-to-end evaluation pipeline from question to metrics
- **ğŸ“ˆ MLflow Integration**: Complete experiment tracking and reproducibility  
- **ğŸ¤– Judge Model**: GPT-4 as reference for generating expected responses
- **ğŸ›ï¸ Configurable**: Flexible model parameters and evaluation settings
- **ğŸ“ Comprehensive Logging**: Detailed logging for debugging and monitoring
- **ğŸ”§ Extensible**: Easy to add new metrics and evaluation methods

## ğŸ”§ Configuration Options

### LLMEvaluator Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `openai_api_key` | str | Required | OpenAI API key for judge model |
| `openai_model` | str | "gpt-4" | OpenAI model to use as judge |
| `teacher_model_temperature` | float | 0.7 | Temperature for judge model responses |
| `teacher_model_max_tokens` | int | 1500 | Maximum tokens for judge responses |

### Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `OPENAI_API_KEY` | âœ… | OpenAI API key for judge model |
| `OLLAMA_BASE_URL` | âœ… | Ollama server endpoint |
| `OLLAMA_MODEL` | âœ… | Ollama model name |
| `MLFLOW_TRACKING_URI` | âœ… | MLflow tracking server URI |
| `MLFLOW_EXPERIMENT_NAME` | âœ… | MLflow experiment name |
| `OPENAI_MODEL` | âŒ | OpenAI model name (default: gpt-4) |

## ğŸ› Troubleshooting

### Common Issues

1. **Import Error**: Ensure file names use underscores (`inference_tracking.py`) not hyphens
2. **MLflow Connection**: Verify MLflow server is running and accessible
3. **Ollama Connection**: Check Ollama server status and model availability
4. **OpenAI API**: Verify API key has sufficient credits and correct permissions

### Debug Mode

Enable detailed logging by modifying the logging configuration:

```python
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ¤ Contributing

Feel free to contribute by:
- Adding new evaluation metrics
- Implementing async/parallel processing
- Enhancing error handling
- Adding support for other LLM providers

## ğŸ“„ License

This project is open source and available under the MIT License.